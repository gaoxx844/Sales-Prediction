{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load packages\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd \n",
    "#from sklearn.preprocessing import StandardScaler \n",
    "import numpy as np\n",
    "from sklearn.metrics import make_scorer, mean_squared_error, confusion_matrix, classification_report, roc_curve, auc\n",
    "from sklearn import metrics, svm, neighbors, SVR, tree\n",
    "from sklearn.linear_model import LinearRegression, LassoCV, Lasso\n",
    "from sklearn.model_selection  import cross_val_score, train_test_split, cross_validate, cross_val_score,KFold,RandomizedSearchCV,RepeatedKFold, GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import confusion_matrix, r2_score\n",
    "from sklearn.svm import \n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up data\n",
    "df=pd.read_excel('C:/Users/yidan/Desktop/fall/PA/hw3/HW3.xlsx')\n",
    "\n",
    "#filtered on the purchase with 1, which represent those who have made a purchase\n",
    "restricted = df[df['Purchase']==1]\n",
    "\n",
    "# Data prep: I dropped the Purchase, Sending columns for X and \n",
    "# include only Spending column for predicting feature\n",
    "X=restricted.drop(['Purchase', 'Spending'], axis=1)\n",
    "y=restricted[['Spending']]\n",
    "#split the train and test dataset by 70% - 30% proportion\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression MSE: 25197.158355\n",
      "Linear Regression R2: 0.482771\n"
     ]
    }
   ],
   "source": [
    "#linear regression\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import cross_val_score\n",
    "regr = LinearRegression()\n",
    "line = regr.fit(X_train, np.ravel(y_train)) \n",
    "\n",
    "#Techniques: MSE, R^2. For continuous dependent variables, \n",
    "#Accuracy, recall and precision are no longer valid on evaluating model performance. \n",
    "#Instead, I look on MSE (as to the information it presents, is equivalent to RMSE in this case), \n",
    "#which tells the cumulative variances of the prediction compared to ground truth and \n",
    "#is more informative than MAE and MAPE, and R^2, \n",
    "#which indicates who well the model can explain the dataset. \n",
    "mse = metrics.mean_squared_error(y_test, line.predict(X_test))\n",
    "print('Linear Regression MSE: %f'%mse)\n",
    "r2 = r2_score(y_test, line.predict(X_test))\n",
    "print('Linear Regression R2: %f'%r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3752463508227345 0.3752463508227345\n",
      "Linear Regression R2: 0.375246\n"
     ]
    }
   ],
   "source": [
    "#lasso CV with even lower score\n",
    "lasso = linear_model.LassoCV(cv=10)\n",
    "Lline = lasso.fit(X_train, np.ravel(y_train)) \n",
    "Lr2 = r2_score(y_test, Lline.predict(X_test)) #ground truth, pred value\n",
    "print('Linear Regression R2: %f'%Lr2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best score: 44018.415671\n",
      "best parameters:\n",
      "n_neighbors: 1\n",
      "weights: uniform\n",
      "test score: -0.198022\n"
     ]
    }
   ],
   "source": [
    "#k-NN\n",
    "k_fold = KFold(n_splits=10)\n",
    "clf_KNN = neighbors.KNeighborsRegressor()\n",
    "weight_options = [\"uniform\", \"distance\"]\n",
    "params = {'n_neighbors':range(1,20), 'weights':weight_options}\n",
    "scoring_fnc = make_scorer(mean_squared_error)\n",
    "grid = GridSearchCV(clf_KNN, param_grid=params,scoring=scoring_fnc,cv=k_fold)\n",
    "grid = grid.fit(X_train, y_train)\n",
    "clf_KNN = grid.best_estimator_\n",
    "\n",
    "print('best score: %f'%grid.best_score_)\n",
    "print('best parameters:')\n",
    "for key in params.keys():\n",
    "    print('%s: %s'%(key, clf_KNN.get_params()[key]))\n",
    "print('test score: %f'%clf_KNN.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:19:47] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[12:19:47] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[12:19:47] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[12:19:48] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "XGBoost MSE:12578.80486955328\n",
      "XGBoost r-squared:0.6428001900770023\n",
      "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
      "             importance_type='gain', learning_rate=0.1, max_delta_step=0,\n",
      "             max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
      "             n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n",
      "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
      "             silent=None, subsample=1, verbosity=1)\n"
     ]
    }
   ],
   "source": [
    "#XGBoost\n",
    "import xgboost as xgb\n",
    "data_dmatrix = xgb.DMatrix(data=X,label=y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "xg_reg = xgb.XGBRegressor()\n",
    "\n",
    "params = {\"objective\":\"reg:linear\",'colsample_bytree': 0.3,'learning_rate': 0.1,\n",
    "                'max_depth': 5, 'alpha': 10}\n",
    "#scoring_fnc = make_scorer(mean_squared_error)\n",
    "#cv_results = GridSearchCV(xg_reg, param_grid=params,scoring=scoring_fnc,cv=3)\n",
    "cv_results = xgb.cv(dtrain=data_dmatrix, params=params, nfold=3,\n",
    "                    num_boost_round=50,early_stopping_rounds=10,metrics=\"rmse\", as_pandas=True, seed=123)\n",
    "\n",
    "xg_reg = xg_reg.fit(X_train,y_train)\n",
    "#clf_xg = grid.best_estimator_\n",
    "\n",
    "mse = metrics.mean_squared_error(y_test, xg_reg.predict(X_test))\n",
    "r2 = metrics.r2_score(y_test, xg_reg.predict(X_test))\n",
    "#for key in params.keys():\n",
    "#    print('%s: %s'%(key, clf_xg.get_params()[key]))\n",
    "print(f'XGBoost MSE:{mse}')\n",
    "print(f'XGBoost r-squared:{r2}')\n",
    "print(xg_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\"objective\":\"reg:linear\",'colsample_bytree': 0.3,'learning_rate': 0.1,\n",
    "                'max_depth': 5, 'alpha': 10}\n",
    "\n",
    "cv_results = xgb.cv(dtrain=data_dmatrix, params=params, nfold=3,\n",
    "                    num_boost_round=50,early_stopping_rounds=10,metrics=\"rmse\", as_pandas=True, seed=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one possible reason for the high mean_squared_error and low R-squared rate is that when\n",
    "#the records of people purchasing nothing is included, the '0' amount in the purchasing \n",
    "#item will influence the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SVR\n",
    "from sklearn.svm import SVR\n",
    "clf_SVR = SVR()\n",
    "C=[0.01, 1, 100]\n",
    "gamma=[100,1,0.01]\n",
    "kernel=['linear','rbf']\n",
    "parameters = {'C': C, 'gamma':gamma, 'kernel':kernel}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##GridSearch cross validation\n",
    "scoring_fnc = make_scorer(mean_squared_error)\n",
    "k_fold = KFold(n_splits=5)\n",
    "grid = RandomizedSearchCV(clf_SVR, param_distributions=parameters,scoring=scoring_fnc,cv=k_fold)\n",
    "\n",
    "grid = grid.fit(X_train, np.ravel(y_train))\n",
    "clf_SVR = grid.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##result\n",
    "print('best score: %f'%grid.best_score_)\n",
    "print('best parameters:')\n",
    "for key in params.keys():\n",
    "    print('%s: %s'%(key, clf_SVR.get_params()[key]))\n",
    "print('test score: %f'%clf_SVR.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hyperparameter tuning for KNN:\n",
      "Fitting 15 folds for each of 57 candidates, totalling 855 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 855 out of 855 | elapsed:    7.8s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best KNN model: {'n_neighbors': 18, 'p': 3}\n",
      "\n",
      "Hyperparameter tuning for DT:\n",
      "Fitting 15 folds for each of 12 candidates, totalling 180 fits\n",
      "Best DT model: {'max_depth': 3, 'min_samples_split': 2}\n",
      "MSE of KNN is: 28332.03218986574\n",
      "MSE of DT is: 21277.984762829623\n",
      "MAE and r-squared KNN are: 95.04128240740742, 0.21802135973269554\n",
      "MAE and r-squared DT are: 80.95318195758557, 0.41271669180096393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 180 out of 180 | elapsed:    0.8s finished\n"
     ]
    }
   ],
   "source": [
    "#KNN & Reg Tree \n",
    "models = {\n",
    "          'KNN': KNeighborsRegressor(),\n",
    "          'DT': tree.DecisionTreeRegressor()}\n",
    "\n",
    "models_parameters = {\n",
    "                     'KNN': {'n_neighbors': range(1,20), \n",
    "                             'p': [1, 2, 3]}, #1：manhattan_distance 2：euclidean_distance\n",
    "                     'DT': {'max_depth': [2, 3, 4], \n",
    "                            'min_samples_split': [2, 3, 4, 5]}}\n",
    "                \n",
    "cv_method = RepeatedKFold(n_splits=5, n_repeats=3, random_state=999)\n",
    "\n",
    "fitted_models = {} # this creates an empty dictionary\n",
    "for m in models: # this will loop over the dictionary keys\n",
    "    print(f'\\nHyperparameter tuning for {m}:')\n",
    "    gs = GridSearchCV(estimator=models[m], \n",
    "                      param_grid=models_parameters[m], \n",
    "                      cv=cv_method,\n",
    "                      verbose=1, \n",
    "                      scoring='neg_mean_squared_error')\n",
    "    gs.fit(X_train, y_train)\n",
    "    fitted_models[m] = gs\n",
    "    print(f'Best {m} model: {gs.best_params_}')\n",
    "\n",
    "#MSE\n",
    "from sklearn import metrics\n",
    "for m in fitted_models:\n",
    "    t_pred = fitted_models[m].predict(X_test)\n",
    "    mse = metrics.mean_squared_error(y_test, t_pred)\n",
    "    print(f'MSE of {m} is: {mse}')\n",
    "\n",
    "#r^2, MAE\n",
    "from sklearn import metrics\n",
    "for m in fitted_models:\n",
    "    t_pred = fitted_models[m].predict(X_test)\n",
    "    r2 = metrics.r2_score(y_test, t_pred)\n",
    "    print(f'MAE and r-squared {m} are: {mae}, {r2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NNT\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "def create_model(activation='relu', nb_hidden=10):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(nb_hidden, input_dim=23, activation=activation))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error',optimizer='adam',metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "activations = ['relu', 'tanh', 'sigmoid', 'linear']\n",
    "nb_hiddens = np.array([100, 200])\n",
    "\n",
    "param_grid = dict(activation=activations, nb_hidden=nb_hiddens)\n",
    "model = KerasRegressor(build_fn=create_model, epochs=20, batch_size=256, verbose=0)\n",
    "scoring_fnc = make_scorer(mean_squared_error)\n",
    "clf = GridSearchCV(estimator=model, param_grid=param_grid, cv=4, scoring=scoring_fnc)\n",
    "res = clf.fit(X_train, y_train)\n",
    "\n",
    "#results\n",
    "print(f'NNT best-score ：{res.best_score_}')\n",
    "print(f'best hyperparameter{res.best_params_}')\n",
    "mse = metrics.mean_squared_error(y_test, res.predict(X_test))\n",
    "r2 = metrics.r2_score(y_test, res.predict(X_test))\n",
    "print(f'NNT MSE:{mse}')\n",
    "print(f'NNT r-squared:{r2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#XGBoost\n",
    "import xgboost as xgb\n",
    "data_dmatrix = xgb.DMatrix(data=X,label=y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "xg_reg = xgb.XGBRegressor(objective ='reg:linear', colsample_bytree = 0.3, learning_rate = 0.1,\n",
    "                max_depth = 5, alpha = 10, n_estimators = 10)\n",
    "\n",
    "xg_reg.fit(X_train,y_train)\n",
    "preds = xg_reg.predict(X_test)\n",
    "\n",
    "mse = metrics.mean_squared_error(y_test, res.predict(X_test))\n",
    "r2 = metrics.r2_score(y_test, res.predict(X_test))\n",
    "print(xg_reg)\n",
    "print(f'NNT MSE:{mse}')\n",
    "print(f'NNT r-squared:{r2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\"objective\":\"reg:linear\",'colsample_bytree': 0.3,'learning_rate': 0.1,\n",
    "                'max_depth': 5, 'alpha': 10}\n",
    "\n",
    "cv_results = xgb.cv(dtrain=data_dmatrix, params=params, nfold=3,\n",
    "                    num_boost_round=50,early_stopping_rounds=10,metrics=\"rmse\", as_pandas=True, seed=123)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
